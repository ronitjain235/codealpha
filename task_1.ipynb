{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvOe88QyKOE3/4XYiXhB+2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronitjain235/codealpha/blob/main/task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGMiYkjuuhvu",
        "outputId": "4a35b787-1be0-4ffa-e614-d15473e77fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Scraping page 1 ...\n",
            "ğŸ”„ Scraping page 2 ...\n",
            "ğŸ”„ Scraping page 3 ...\n",
            "ğŸ”„ Scraping page 4 ...\n",
            "ğŸ”„ Scraping page 5 ...\n",
            "ğŸ”„ Scraping page 6 ...\n",
            "ğŸ”„ Scraping page 7 ...\n",
            "ğŸ”„ Scraping page 8 ...\n",
            "ğŸ”„ Scraping page 9 ...\n",
            "ğŸ”„ Scraping page 10 ...\n",
            "ğŸ”„ Scraping page 11 ...\n",
            "ğŸ”„ Scraping page 12 ...\n",
            "ğŸ”„ Scraping page 13 ...\n",
            "ğŸ”„ Scraping page 14 ...\n",
            "ğŸ”„ Scraping page 15 ...\n",
            "ğŸ”„ Scraping page 16 ...\n",
            "ğŸ”„ Scraping page 17 ...\n",
            "ğŸ”„ Scraping page 18 ...\n",
            "ğŸ”„ Scraping page 19 ...\n",
            "ğŸ”„ Scraping page 20 ...\n",
            "ğŸ”„ Scraping page 21 ...\n",
            "ğŸ”„ Scraping page 22 ...\n",
            "ğŸ”„ Scraping page 23 ...\n",
            "ğŸ”„ Scraping page 24 ...\n",
            "ğŸ”„ Scraping page 25 ...\n",
            "ğŸ”„ Scraping page 26 ...\n",
            "ğŸ”„ Scraping page 27 ...\n",
            "ğŸ”„ Scraping page 28 ...\n",
            "ğŸ”„ Scraping page 29 ...\n",
            "ğŸ”„ Scraping page 30 ...\n",
            "ğŸ”„ Scraping page 31 ...\n",
            "ğŸ”„ Scraping page 32 ...\n",
            "ğŸ”„ Scraping page 33 ...\n",
            "ğŸ”„ Scraping page 34 ...\n",
            "ğŸ”„ Scraping page 35 ...\n",
            "ğŸ”„ Scraping page 36 ...\n",
            "ğŸ”„ Scraping page 37 ...\n",
            "ğŸ”„ Scraping page 38 ...\n",
            "ğŸ”„ Scraping page 39 ...\n",
            "ğŸ”„ Scraping page 40 ...\n",
            "ğŸ”„ Scraping page 41 ...\n",
            "ğŸ”„ Scraping page 42 ...\n",
            "ğŸ”„ Scraping page 43 ...\n",
            "ğŸ”„ Scraping page 44 ...\n",
            "ğŸ”„ Scraping page 45 ...\n",
            "ğŸ”„ Scraping page 46 ...\n",
            "ğŸ”„ Scraping page 47 ...\n",
            "ğŸ”„ Scraping page 48 ...\n",
            "ğŸ”„ Scraping page 49 ...\n",
            "ğŸ”„ Scraping page 50 ...\n",
            "âœ… Scraping completed. Data saved to scraped_books_full.csv\n",
            "ğŸ“Š Total books scraped: 1000\n"
          ]
        }
      ],
      "source": [
        "# Task 1: Web Scraping with BeautifulSoup (Google Colab Version)\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Base URL\n",
        "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
        "\n",
        "# Lists to store scraped data\n",
        "book_titles = []\n",
        "book_prices = []\n",
        "book_availability = []\n",
        "book_ratings = []\n",
        "\n",
        "# Function to convert rating text into numbers\n",
        "def get_rating(rating_class):\n",
        "    ratings_map = {\n",
        "        \"One\": 1,\n",
        "        \"Two\": 2,\n",
        "        \"Three\": 3,\n",
        "        \"Four\": 4,\n",
        "        \"Five\": 5\n",
        "    }\n",
        "    for word, num in ratings_map.items():\n",
        "        if word in rating_class:\n",
        "            return num\n",
        "    return None\n",
        "\n",
        "# Loop through all 50 pages\n",
        "for page in range(1, 51):\n",
        "    print(f\"ğŸ”„ Scraping page {page} ...\")\n",
        "\n",
        "    response = requests.get(base_url.format(page))\n",
        "    if response.status_code != 200:\n",
        "        print(f\"âŒ Page {page} not found. Stopping...\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    books = soup.find_all(\"article\", class_=\"product_pod\")\n",
        "\n",
        "    for book in books:\n",
        "        title = book.h3.a[\"title\"]\n",
        "        price = book.find(\"p\", class_=\"price_color\").text.strip()\n",
        "        availability = book.find(\"p\", class_=\"instock availability\").text.strip()\n",
        "        rating_class = book.find(\"p\", class_=\"star-rating\")[\"class\"]\n",
        "\n",
        "        # Clean price (remove Â£ and convert to float)\n",
        "        price_clean = float(price.replace(\"Â£\", \"\").replace(\"Ã‚\", \"\"))\n",
        "\n",
        "        # Get rating number\n",
        "        rating = get_rating(rating_class)\n",
        "\n",
        "        book_titles.append(title)\n",
        "        book_prices.append(price_clean)\n",
        "        book_availability.append(availability)\n",
        "        book_ratings.append(rating)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Title\": book_titles,\n",
        "    \"Price (Â£)\": book_prices,\n",
        "    \"Availability\": book_availability,\n",
        "    \"Rating (1-5)\": book_ratings\n",
        "})\n",
        "\n",
        "# Save CSV\n",
        "df.to_csv(\"scraped_books_full.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"âœ… Scraping completed. Data saved to scraped_books_full.csv\")\n",
        "print(f\"ğŸ“Š Total books scraped: {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"scraped_books_full.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TiFOy0psvht6",
        "outputId": "49822ab3-8b68-4669-b6c2-3aae896dd7f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_471cbf96-e45a-417a-b9bf-30298697c0a3\", \"scraped_books_full.csv\", 57382)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}